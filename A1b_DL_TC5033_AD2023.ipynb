{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TC 5033\n",
    "## Deep Learning\n",
    "## Fully Connected Deep Neural Networks\n",
    "\n",
    "### Team 30\n",
    "- A01796272 - Luis Antonio Ramirez Martinez\n",
    "- A01796323 - Benjamin Cisneros Barraza\n",
    "- A01796363 - Arthur Jafed Zizumbo Velasco\n",
    "- A01796937 - Sandra Luz Cervantes Espinoza\n",
    "\n",
    "#### Activity 1b: Implementing a Fully Connected Network for Kaggle ASL Dataset\n",
    "\n",
    "- Objective\n",
    "\n",
    "The aim of this part of the activity is to apply your understanding of Fully Connected Networks by implementing a multilayer network for the [Kaggle ASL (American Sign Language) dataset](https://www.kaggle.com/datasets/grassknoted/asl-alphabet). While you have been provided with a complete solution for a Fully Connected Network using Numpy for the MNIST dataset, you are encouraged to try to come up with the solution.\n",
    "\n",
    "- Instructions\n",
    "\n",
    "    This activity requires submission in teams of 3 or 4 members. Submissions from smaller or larger teams will not be accepted unless prior approval has been granted (only due to exceptional circumstances). While teamwork is encouraged, each member is expected to contribute individually to the assignment. The final submission should feature the best arguments and solutions from each team member. Only one person per team needs to submit the completed work, but it is imperative that the names of all team members are listed in a Markdown cell at the very beginning of the notebook (either the first or second cell). Failure to include all team member names will result in the grade being awarded solely to the individual who submitted the assignment, with zero points given to other team members (no exceptions will be made to this rule).\n",
    "\n",
    "    Load and Preprocess Data: You are provided a starter code to load the data. Be sure to understand the code.\n",
    "\n",
    "    Review MNIST Notebook (Optional): Before diving into this activity, you have the option to revisit the MNIST example to refresh your understanding of how to build a Fully Connected Network using Numpy.\n",
    "\n",
    "    Start Fresh: Although you can refer to the MNIST solution at any point, try to implement the network for the ASL dataset on your own. This will reinforce your learning and understanding of the architecture and mathematics involved.\n",
    "\n",
    "    Implement Forward and Backward Pass: Write the code to perform the forward and backward passes, keeping in mind the specific challenges and characteristics of the ASL dataset.\n",
    "    \n",
    "     Design the Network: Create the architecture of the Fully Connected Network tailored for the ASL dataset. Choose the number of hidden layers, neurons, and hyperparameters judiciously.\n",
    "\n",
    "    Train the Model: Execute the training loop, ensuring to track performance metrics such as loss and accuracy.\n",
    "\n",
    "    Analyze and Document: Use Markdown cells to document in detail the choices you made in terms of architecture and hyperparameters, you may use figures, equations, etc to aid in your explanations. Include any metrics that help justify these choices and discuss the model's performance.  \n",
    "\n",
    "- Evaluation Criteria\n",
    "\n",
    "    - Code Readability and Comments\n",
    "    - Appropriateness of chosen architecture and hyperparameters for the ASL dataset\n",
    "    - Performance of the model on the ASL dataset (at least 70% acc)\n",
    "    - Quality of Markdown documentation\n",
    "\n",
    "- Submission\n",
    "\n",
    "Submit this Jupyter Notebook in canvas with your complete solution, ensuring your code is well-commented and includes Markdown cells that explain your design choices, results, and any challenges you encountered.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports and Jupyter Magics\n",
    "This cell imports the core scientific Python stack and enables convenient Jupyter features:\n",
    "- **NumPy** for numerical computing, **pandas** for tabular data, **Matplotlib** for plotting, **OpenCV** for image I/O/processing, and **os** for filesystem utilities.\n",
    "- IPython magics: `autoreload` (automatically reloads edited modules) and `matplotlib inline` (renders plots in the notebook output)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import string\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2 as cv\n",
    "import os\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "#################################\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load ASL SignMNIST CSVs (Train/Validation)\n",
    "Loads the Kaggle ASL dataset from CSV files:\n",
    "\n",
    "- `sign_mnist_train.csv` â†’ `train_df`\n",
    "- `sign_mnist_valid.csv` â†’ `valid_df`\n",
    "\n",
    "Each row includes a **label** (class) and **784 pixel values** (flattened 28Ã—28 grayscale image)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = './asl_data'\n",
    "train_df = pd.read_csv(os.path.join(DATA_PATH, 'sign_mnist_train.csv'))\n",
    "valid_df = pd.read_csv(os.path.join(DATA_PATH, 'sign_mnist_valid.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>pixel1</th>\n",
       "      <th>pixel2</th>\n",
       "      <th>pixel3</th>\n",
       "      <th>pixel4</th>\n",
       "      <th>pixel5</th>\n",
       "      <th>pixel6</th>\n",
       "      <th>pixel7</th>\n",
       "      <th>pixel8</th>\n",
       "      <th>pixel9</th>\n",
       "      <th>...</th>\n",
       "      <th>pixel775</th>\n",
       "      <th>pixel776</th>\n",
       "      <th>pixel777</th>\n",
       "      <th>pixel778</th>\n",
       "      <th>pixel779</th>\n",
       "      <th>pixel780</th>\n",
       "      <th>pixel781</th>\n",
       "      <th>pixel782</th>\n",
       "      <th>pixel783</th>\n",
       "      <th>pixel784</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>107</td>\n",
       "      <td>118</td>\n",
       "      <td>127</td>\n",
       "      <td>134</td>\n",
       "      <td>139</td>\n",
       "      <td>143</td>\n",
       "      <td>146</td>\n",
       "      <td>150</td>\n",
       "      <td>153</td>\n",
       "      <td>...</td>\n",
       "      <td>207</td>\n",
       "      <td>207</td>\n",
       "      <td>207</td>\n",
       "      <td>207</td>\n",
       "      <td>206</td>\n",
       "      <td>206</td>\n",
       "      <td>206</td>\n",
       "      <td>204</td>\n",
       "      <td>203</td>\n",
       "      <td>202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6</td>\n",
       "      <td>155</td>\n",
       "      <td>157</td>\n",
       "      <td>156</td>\n",
       "      <td>156</td>\n",
       "      <td>156</td>\n",
       "      <td>157</td>\n",
       "      <td>156</td>\n",
       "      <td>158</td>\n",
       "      <td>158</td>\n",
       "      <td>...</td>\n",
       "      <td>69</td>\n",
       "      <td>149</td>\n",
       "      <td>128</td>\n",
       "      <td>87</td>\n",
       "      <td>94</td>\n",
       "      <td>163</td>\n",
       "      <td>175</td>\n",
       "      <td>103</td>\n",
       "      <td>135</td>\n",
       "      <td>149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>187</td>\n",
       "      <td>188</td>\n",
       "      <td>188</td>\n",
       "      <td>187</td>\n",
       "      <td>187</td>\n",
       "      <td>186</td>\n",
       "      <td>187</td>\n",
       "      <td>188</td>\n",
       "      <td>187</td>\n",
       "      <td>...</td>\n",
       "      <td>202</td>\n",
       "      <td>201</td>\n",
       "      <td>200</td>\n",
       "      <td>199</td>\n",
       "      <td>198</td>\n",
       "      <td>199</td>\n",
       "      <td>198</td>\n",
       "      <td>195</td>\n",
       "      <td>194</td>\n",
       "      <td>195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>211</td>\n",
       "      <td>211</td>\n",
       "      <td>212</td>\n",
       "      <td>212</td>\n",
       "      <td>211</td>\n",
       "      <td>210</td>\n",
       "      <td>211</td>\n",
       "      <td>210</td>\n",
       "      <td>210</td>\n",
       "      <td>...</td>\n",
       "      <td>235</td>\n",
       "      <td>234</td>\n",
       "      <td>233</td>\n",
       "      <td>231</td>\n",
       "      <td>230</td>\n",
       "      <td>226</td>\n",
       "      <td>225</td>\n",
       "      <td>222</td>\n",
       "      <td>229</td>\n",
       "      <td>163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>12</td>\n",
       "      <td>164</td>\n",
       "      <td>167</td>\n",
       "      <td>170</td>\n",
       "      <td>172</td>\n",
       "      <td>176</td>\n",
       "      <td>179</td>\n",
       "      <td>180</td>\n",
       "      <td>184</td>\n",
       "      <td>185</td>\n",
       "      <td>...</td>\n",
       "      <td>92</td>\n",
       "      <td>105</td>\n",
       "      <td>105</td>\n",
       "      <td>108</td>\n",
       "      <td>133</td>\n",
       "      <td>163</td>\n",
       "      <td>157</td>\n",
       "      <td>163</td>\n",
       "      <td>164</td>\n",
       "      <td>179</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 785 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   label  pixel1  pixel2  pixel3  pixel4  pixel5  pixel6  pixel7  pixel8  \\\n",
       "0      3     107     118     127     134     139     143     146     150   \n",
       "1      6     155     157     156     156     156     157     156     158   \n",
       "2      2     187     188     188     187     187     186     187     188   \n",
       "3      2     211     211     212     212     211     210     211     210   \n",
       "4     12     164     167     170     172     176     179     180     184   \n",
       "\n",
       "   pixel9  ...  pixel775  pixel776  pixel777  pixel778  pixel779  pixel780  \\\n",
       "0     153  ...       207       207       207       207       206       206   \n",
       "1     158  ...        69       149       128        87        94       163   \n",
       "2     187  ...       202       201       200       199       198       199   \n",
       "3     210  ...       235       234       233       231       230       226   \n",
       "4     185  ...        92       105       105       108       133       163   \n",
       "\n",
       "   pixel781  pixel782  pixel783  pixel784  \n",
       "0       206       204       203       202  \n",
       "1       175       103       135       149  \n",
       "2       198       195       194       195  \n",
       "3       225       222       229       163  \n",
       "4       157       163       164       179  \n",
       "\n",
       "[5 rows x 785 columns]"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importar Images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split Labels and Features (X/y)\n",
    "Separates the **label** column into NumPy arrays `y_train` and `y_val`, removes it from the DataFrames, and converts the remaining pixel columns into float32 NumPy matrices `x_train` and `x_val`.\n",
    "Resulting shapes:\n",
    "- `x_*`: (num_samples, 784)\n",
    "- `y_*`: (num_samples,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = np.array(train_df['label'])\n",
    "y_val = np.array(valid_df['label'])\n",
    "del train_df['label']\n",
    "del valid_df['label']\n",
    "x_train = train_df.values.astype(np.float32)\n",
    "x_val = valid_df.values.astype(np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper: Split Validation into New Validation + Test\n",
    "Defines `split_val_test(x, y, pct=0.5, shuffle=True)` to split an input set into:\n",
    "\n",
    "- **test** portion of size `pct`\n",
    "- **validation** portion with the remainder\n",
    "\n",
    "Optionally shuffles indices to avoid ordering bias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def split_val_test(x, y, pct=0.5, shuffle=True):\n",
    "    '''\n",
    "    Create a function that will allow you to split the previously loaded validation set\n",
    "    into valition and test.\n",
    "    '''\n",
    "    x = np.array(x)\n",
    "    y = np.array(y)\n",
    "\n",
    "    n = len(y)\n",
    "    idx = np.arange(n)\n",
    "    if shuffle:\n",
    "        np.random.shuffle(idx)\n",
    "    x = x[idx]\n",
    "    y = y[idx]\n",
    "\n",
    "    n_test = int(n * pct)\n",
    "    x_test = x[:n_test]\n",
    "    y_test = y[:n_test]\n",
    "    x_val  = x[n_test:]\n",
    "    y_val  = y[n_test:]\n",
    "    return x_val, y_val, x_test, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Test Set from the Existing Validation\n",
    "Uses `split_val_test` on the current validation set to obtain:\n",
    "- a **new** (smaller) validation set (`x_val`, `y_val`)\n",
    "- a **held-out test set** (`x_test`, `y_test`)\n",
    "\n",
    "This provides a clean test set for final evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_val, y_val, x_test, y_test = split_val_test(x_val, y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Class Names (Alphabet Without 'j' and 'z')\n",
    "Builds the list of lowercase letters and removes **'j'** and **'z'** because they require motion and are not captured in static images in the classic SignMNIST dataset, leaving **24 classes**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24\n"
     ]
    }
   ],
   "source": [
    "alphabet=list(string.ascii_lowercase)\n",
    "alphabet.remove('j')\n",
    "alphabet.remove('z')\n",
    "print(len(alphabet))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Global Standardization (Z-Score) Using Train Statistics\n",
    "Computes the **global mean and std** *only on the training data* and standardizes:\n",
    "$$x' = \\frac{x - \\mu_{\\text{train}}}{\\sigma_{\\text{train}}}$$\n",
    "Applies the same transformation to validation and test to avoid data leakage and stabilize training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(27455, 784) (27455,) (3586, 784) (3586,) (3586, 784) (3586,)\n"
     ]
    }
   ],
   "source": [
    "x_mean = x_train.mean()\n",
    "x_std  = x_train.std() + 1e-8\n",
    "\n",
    "x_train = (x_train - x_mean) / x_std\n",
    "x_val   = (x_val   - x_mean) / x_std\n",
    "x_test  = (x_test  - x_mean) / x_std\n",
    "\n",
    "print(x_train.shape, y_train.shape, x_val.shape, y_val.shape, x_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utility: Plot a 28Ã—28 Grayscale Image\n",
    "Defines `plot_number(img_28x28)` to visualize a single image (2D array) in grayscale without axes. Handy for quick sanity checks on samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_number(img_28x28):\n",
    "    plt.figure()\n",
    "    plt.imshow(img_28x28, cmap='gray')\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show a Random Training Sample\n",
    "Randomly selects an index, reshapes the flattened vector to **28Ã—28**, displays the image, and prints its numeric label (optionally mapping to a letter). Useful for verifying data integrity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: 5\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "i = np.random.randint(len(x_train))\n",
    "img = x_train[i].reshape(28, 28)\n",
    "label = y_train[i]\n",
    "\n",
    "print(\"Label:\", label)\n",
    "plot_number(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Equations \n",
    "\n",
    "\n",
    "#### ðŸ”¹ 1. First Linear Transformation (Layer 1)\n",
    "\n",
    "The first step is to compute the linear combination of inputs and weights:\n",
    "\n",
    "$$z^1 = W^1 X + b^1$$\n",
    "\n",
    "\n",
    "- $X$: Input data (each column is one sample).\n",
    "- $W^1$: Weight matrix of the first layer.\n",
    "- $b^1$: Bias vector of the first layer.\n",
    "- $z^1$: Pre-activation output.\n",
    "\n",
    "This is the standard affine transformation applied in fully connected neural networks.\n",
    "\n",
    "\n",
    "#### ðŸ”¹ 2. ReLU Activation Function\n",
    "\n",
    "After computing $z^1$, we apply the $ReLU$ activation:\n",
    "\n",
    "\n",
    "$$a^1 = ReLU(z^1) $$\n",
    "\n",
    "\n",
    "Where $ReLU$ is defined as:\n",
    "\n",
    "$$ReLU(x) = \\max(0, x)$$\n",
    "\n",
    "\n",
    "$ReLU$ introduces **non-linearity** into the model, allowing it to learn complex patterns.\n",
    "It keeps positive values unchanged and zeroes out the negative ones.\n",
    "\n",
    "\n",
    "#### ðŸ”¹ 3. Second Linear Transformation (Layer 2)\n",
    "\n",
    "The output of the ReLU layer is passed through another linear transformation:\n",
    "\n",
    "\n",
    "$$z^2 = W^2 a^1 + b^2$$\n",
    "\n",
    "\n",
    "- $W^2$, $b^2$: Parameters of the second layer.\n",
    "- $z^2$: Logits (raw scores before softmax).\n",
    "\n",
    "These logits represent unnormalized scores for each class.\n",
    "\n",
    "\n",
    "#### ðŸ”¹ 4. Softmax Function (Class Probabilities)\n",
    "\n",
    "The logits are converted into probabilities using the softmax function:\n",
    "\n",
    "\n",
    "$$\\hat{y} = \\frac{e^{z^{2_k}}}{\\sum_j{e^{z_j}}}$$\n",
    "\n",
    "\n",
    "Softmax ensures:\n",
    "- All outputs are between 0 and 1.\n",
    "- Probabilities across all classes sum to 1.\n",
    "- Higher logits correspond to higher probabilities.\n",
    "\n",
    "\n",
    "#### ðŸ”¹ 5. Cross-Entropy Loss (Per Sample)\n",
    "\n",
    "For each individual sample, the loss is computed as:\n",
    "\n",
    "\n",
    "$$ \\mathcal{L}(\\hat{y}^{i}, y^{i}) =  - y^{i}  \\ln(\\hat{y}^{i}) = -\\ln(\\hat{y}^i)$$\n",
    "\n",
    "\n",
    "Where:\n",
    "- $y^i$ is the true class (one-hot or class index).\n",
    "- $\\hat{y}^i$ is the predicted probability of the true class.\n",
    "\n",
    "This loss punishes confident but wrong predictions.\n",
    "\n",
    "\n",
    "#### ðŸ”¹ 6. Cost Function (Mean Loss)\n",
    "\n",
    "The total cost over all samples is the average cross-entrop\n",
    "\n",
    "$$ \\mathcal{J}(w, b) =  \\frac{1}{num\\_samples} \\sum_{i=1}^{num\\_samples}-\\ln(\\hat{y}^{i})$$\n",
    "\n",
    "\n",
    "This is the value minimized during training.\n",
    "Lower cost indicates better performance of the model.\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸŽ¯ Summary of Forward Pass\n",
    "\n",
    "1. Compute first linear output: $z^1$  \n",
    "2. Apply ReLU: $a^1$  \n",
    "3. Compute logits: $z^2$  \n",
    "4. Apply softmax: $\\hat{y}$  \n",
    "5. Compute cross-entropy loss  \n",
    "\n",
    "This pipeline transforms raw input data into meaningful class probabilities\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Additional Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mini-batch Generator\n",
    "Defines `create_minibatches(mb_size, x, y, shuffle=True)` that yields `(X_batch, y_batch)` slices.\n",
    "\n",
    "- Shuffles indices to randomize sample order.\n",
    "- Produces the last batch even if it is smaller than `mb_size`.\n",
    "\n",
    "This enables efficient SGD training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_minibatches(mb_size, x, y, shuffle=True):\n",
    "    n = len(y)\n",
    "    idx = np.arange(n)\n",
    "    if shuffle:\n",
    "        np.random.shuffle(idx)\n",
    "    for i in range(0, n, mb_size):\n",
    "        batch_idx = idx[i:i+mb_size]\n",
    "        yield x[batch_idx], y[batch_idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (Optional) ndarray Subclass for Gradients\n",
    "Declares `np_tensor` as a minimal subclass of `np.ndarray` intended to allow attaching attributes like `.grad`. Note: many NumPy operations return base `ndarray`, so attributes may be lostâ€”wrapping with a small `Tensor` class is often safer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "class np_tensor(np.ndarray): pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### View Casting Example\n",
    "Creates a view of a NumPy array as `np_tensor` so it can carry `.grad`. Views share memory with the original array; modifying one affects the other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array([0, 0])\n",
    "b = a.view(np_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classes: Linear, ReLU and Sequential"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Linear Layer (Fully Connected)\n",
    "Implements a dense layer with:\n",
    "- **He (Kaiming) initialization** for weights (suitable for ReLU).\n",
    "- **Forward:** $Z = W @ X + b$ with column-major batches (each column is a sample).\n",
    "- **Backward:** computes gradients for inputs, weights, and biases using the upstream `Z.grad`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear:\n",
    "    \"\"\"\n",
    "    Fully connected (affine) layer implementing:\n",
    "        Z = W @ X + b\n",
    "\n",
    "    Initialization\n",
    "    --------------\n",
    "    - Weights (W) are initialized with Kaiming He (He normal) scaling suitable for ReLU:\n",
    "        std = sqrt(2 / fan_in)  â‡”  randn / sqrt(fan_in / 2)\n",
    "      where fan_in = in_features.\n",
    "    - Biases (b) are initialized to zeros and broadcast across the batch dimension.\n",
    "\n",
    "    Expected Shapes (column-major batches)\n",
    "    --------------------------------------\n",
    "    - W: (out_features, in_features)\n",
    "    - b: (out_features, 1)\n",
    "    - X: (in_features, batch_size)\n",
    "    - Z: (out_features, batch_size)\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    - This implementation assumes tensors support a `.grad` attribute for backpropagation\n",
    "      (e.g., via a NumPy subclass or a lightweight Tensor wrapper).\n",
    "    - Gradients computed in `backward` match standard matrix calculus for MSE/CE-style losses:\n",
    "        dL/dX = W.T @ dL/dZ\n",
    "        dL/dW = dL/dZ @ X.T\n",
    "        dL/db = sum(dL/dZ, axis=1, keepdims=True)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_features, out_features):\n",
    "        # Kaiming He initialization for W: randn / sqrt(in_features/2) -> std = sqrt(2/fan_in)\n",
    "        self.W = (np.random.randn(out_features, in_features) / np.sqrt(in_features/2)).view(np_tensor)\n",
    "        # Bias initialized to zeros; will broadcast across columns (batch dimension)\n",
    "        self.b = (np.zeros((out_features, 1))).view(np_tensor)\n",
    "\n",
    "    def __call__(self, X):  # forward pass of the Linear layer\n",
    "        \"\"\"\n",
    "        Forward pass.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like, shape (in_features, batch_size)\n",
    "            Input activations where each column is a sample.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        Z : array-like, shape (out_features, batch_size)\n",
    "            Linear transformation result Z = W @ X + b\n",
    "            (bias b is broadcast over the batch dimension).\n",
    "        \"\"\"\n",
    "        Z = self.W @ X + self.b\n",
    "        return Z\n",
    "\n",
    "    def backward(self, X, Z):\n",
    "        \"\"\"\n",
    "        Backward pass given upstream gradient stored in `Z.grad` (âˆ‚L/âˆ‚Z).\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like, shape (in_features, batch_size)\n",
    "            The same input that was used in the forward pass.\n",
    "        Z : array-like, shape (out_features, batch_size)\n",
    "            The output from the forward pass; must carry `Z.grad` as the upstream gradient.\n",
    "\n",
    "        Side Effects\n",
    "        ------------\n",
    "        - Populates:\n",
    "            X.grad : (in_features, batch_size)   with  dL/dX = W.T @ dL/dZ\n",
    "            W.grad : (out_features, in_features) with  dL/dW = dL/dZ @ X.T\n",
    "            b.grad : (out_features, 1)           with  dL/db = sum(dL/dZ, axis=1, keepdims=True)\n",
    "        \"\"\"\n",
    "        # Z.grad is defined by the next layer (or by the loss for the final layer)\n",
    "        X.grad = self.W.T @ Z.grad\n",
    "        self.W.grad = (Z.grad @ X.T)\n",
    "        self.b.grad = np.sum(Z.grad, axis=1, keepdims=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ReLU Activation\n",
    "Implements the element-wise ReLU:\n",
    "- **Forward:** $\\max(0, Z)$\n",
    "- **Backward:** masks the upstream gradient with $(Z > 0)$ to obtain $Z.grad$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReLU:\n",
    "    \"\"\"\n",
    "    Rectified Linear Unit (ReLU) activation layer.\n",
    "\n",
    "    This layer applies the element-wise transformation:\n",
    "        ReLU(z) = max(0, z)\n",
    "\n",
    "    Interface\n",
    "    ---------\n",
    "    - Forward pass via __call__(Z) -> A\n",
    "      * Z: pre-activation input (any real-valued array-like)\n",
    "      * A: activation output with the same shape as Z\n",
    "\n",
    "    - Backward pass via backward(Z, A)\n",
    "      * Z: the same tensor passed to the forward pass (pre-activation)\n",
    "      * A: the output tensor produced by the forward pass; it is expected\n",
    "           to carry the upstream gradient in `A.grad` (i.e., âˆ‚L/âˆ‚A)\n",
    "      * This method writes the downstream gradient into `Z.grad`:\n",
    "           âˆ‚L/âˆ‚Z = (Z > 0) âŠ™ âˆ‚L/âˆ‚A\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    - This implementation assumes tensors support a `.grad` attribute\n",
    "      (e.g., via a NumPy subclass or a lightweight Tensor wrapper).\n",
    "    - ReLU's derivative is 1 where Z > 0 and 0 elsewhere (commonly 0 at Z = 0).\n",
    "    \"\"\"\n",
    "\n",
    "    def __call__(self, Z):\n",
    "        # Forward pass: apply ReLU element-wise (clamp negatives to zero)\n",
    "        return np.maximum(0, Z)\n",
    "\n",
    "    def backward(self, Z, A):\n",
    "        # Backward pass: start from upstream gradient âˆ‚L/âˆ‚A\n",
    "        Z.grad = A.grad.copy()\n",
    "\n",
    "        # Mask gradient where the pre-activation Z is <= 0\n",
    "        Z.grad[Z <= 0] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sequential Container\n",
    "Defines a simple model runner that:\n",
    "- **Caches** inputs/outputs per layer during forward (`l0`, `l1`, â€¦).\n",
    "- **Backpropagates** through layers in reverse order using cached tensors.\n",
    "- **Updates** parameters for `Linear` layers via SGD.\n",
    "- **Predicts** the class index for a single column input via `argmax`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sequential_layers:\n",
    "    \"\"\"\n",
    "    A simple sequential container to compose layers into a feed-forward network.\n",
    "\n",
    "    This class chains together a list of layer objects that implement a common\n",
    "    interface:\n",
    "        - Forward pass via `__call__(X) -> Y`\n",
    "        - Backward pass via `backward(inp, out)`, where:\n",
    "            * `inp` is the input tensor passed to the layer during the forward pass\n",
    "            * `out` is the output tensor produced by the layer during the forward pass\n",
    "              and is expected to carry `.grad` set by the next layer (or the loss for the last layer).\n",
    "\n",
    "    The container:\n",
    "      - Runs the forward pass left-to-right, caching each layer's output in `self.outputs`\n",
    "        using keys \"l0\", \"l1\", ..., \"lN\".\n",
    "      - Runs the backward pass right-to-left, calling each layer's `backward` with the\n",
    "        cached input/output tensors.\n",
    "      - Provides a parameter update step for `Linear` layers (SGD), and a convenience\n",
    "        `predict` method for single-column inputs.\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    - This implementation assumes tensors can carry a `.grad` attribute (e.g., via a\n",
    "      NumPy subclass or a lightweight Tensor wrapper).\n",
    "    - The `update` method currently *reassigns* `W` and `b` (not in-place), which can\n",
    "      discard attributes like `.grad` on those arrays. If you rely on persisting `.grad`\n",
    "      across steps, consider in-place updates (e.g., `layer.W -= lr * layer.W.grad`).\n",
    "    - The `predict` method expects a single example shaped as a column vector\n",
    "      `(in_features, 1)` and returns the argmax class index as `int`.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, layers):\n",
    "        \"\"\"\n",
    "        Initialize the sequential container.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        layers : list\n",
    "            Ordered list of layer objects. Each layer must implement:\n",
    "                - `__call__(X)` for forward propagation\n",
    "                - `backward(inp, out)` for backpropagation\n",
    "            Layers like `Linear` are expected to own parameters (e.g., W, b) and\n",
    "            their gradients, while activation layers (e.g., ReLU) are typically\n",
    "            parameter-free.\n",
    "        \"\"\"\n",
    "        # Store the sequence of layers and a cache for intermediate outputs\n",
    "        self.layers = layers\n",
    "        self.outputs = {}\n",
    "\n",
    "    def __call__(self, X):\n",
    "        \"\"\"\n",
    "        Perform a forward pass through all layers and cache intermediate outputs.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like\n",
    "            Network input for the first layer. Conventionally shaped as\n",
    "            `(in_features, batch_size)` so that each column is a sample.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        array-like\n",
    "            The final output of the last layer (e.g., logits).\n",
    "            The method also populates `self.outputs` with:\n",
    "              - 'l0' = X (the original input)\n",
    "              - 'l1' = output of layer 1\n",
    "              - ...\n",
    "              - 'lN' = output of the last layer\n",
    "        \"\"\"\n",
    "        # Forward pass through all layers, caching each layer's output\n",
    "        self.outputs['l0'] = X            # Cache network input as layer 0 output\n",
    "        out = X\n",
    "        for i, layer in enumerate(self.layers, start=1):\n",
    "            out = layer(out)              # Apply layer i to the previous output\n",
    "            self.outputs[f'l{i}'] = out   # Cache current layer output\n",
    "        return out                         # Final output (e.g., logits)\n",
    "\n",
    "    def backward(self):\n",
    "        \"\"\"\n",
    "        Perform a backward pass through all layers in reverse order.\n",
    "\n",
    "        Requirements\n",
    "        ------------\n",
    "        - The final output tensor (cached as `self.outputs['lN']`) must already have\n",
    "          its `.grad` attribute set by the loss function (i.e., dL/dOut).\n",
    "        - Each layer's `backward(inp, out)` is responsible for:\n",
    "            * Reading `out.grad` (upstream gradient)\n",
    "            * Writing `inp.grad` (downstream gradient)\n",
    "            * Accumulating parameter gradients (e.g., `W.grad`, `b.grad`) if the\n",
    "              layer has learnable parameters.\n",
    "        \"\"\"\n",
    "        # Backward pass: iterate layers in reverse order\n",
    "        # Assumes the final output (self.outputs['lN']) already has .grad set by the loss\n",
    "        for i in range(len(self.layers), 0, -1):\n",
    "            layer = self.layers[i-1]              # Current layer (from last to first)\n",
    "            inp = self.outputs[f'l{i-1}']         # Input to this layer during forward\n",
    "            out = self.outputs[f'l{i}']           # Output from this layer during forward\n",
    "            layer.backward(inp, out)              # Propagate gradients through the layer\n",
    "\n",
    "    def update(self, lr):\n",
    "        \"\"\"\n",
    "        Apply a single SGD update step to parameters of supported layers.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        lr : float\n",
    "            Learning rate for the update step. For each `Linear` layer:\n",
    "                W <- W - lr * W.grad\n",
    "                b <- b - lr * b.grad\n",
    "\n",
    "        Notes\n",
    "        -----\n",
    "        - This implementation *reassigns* `W` and `b` with new arrays via `.view(np_tensor)`.\n",
    "          Reassignment can drop previously attached attributes (e.g., `.grad`). If you need\n",
    "          to preserve such attributes, consider performing updates in-place.\n",
    "        \"\"\"\n",
    "        # Parameter update step for layers that have weights/biases (Linear layers)\n",
    "        # NOTE: This reassigns arrays rather than updating in-place; be careful with .grad persistence.\n",
    "        for layer in self.layers:\n",
    "            if isinstance(layer, Linear):\n",
    "                layer.W = (layer.W - lr * layer.W.grad).view(np_tensor)\n",
    "                layer.b = (layer.b - lr * layer.b.grad).view(np_tensor)\n",
    "\n",
    "    def predict(self, x_col):\n",
    "        \"\"\"\n",
    "        Predict the class index for a single input column.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        x_col : array-like\n",
    "            Single example shaped as a column vector `(in_features, 1)`.\n",
    "            The data will be forwarded through the network to produce scores.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        int\n",
    "            The predicted class index given by `argmax` over the class dimension.\n",
    "\n",
    "        Notes\n",
    "        -----\n",
    "        - This method is intended for single-sample inference. For batched inference,\n",
    "          call the instance directly (i.e., `scores = model(X)`) and apply `argmax`\n",
    "          across axis=0 to obtain class predictions for each column.\n",
    "        \"\"\"\n",
    "        # Run a forward pass for a single column input (shape: features x 1)\n",
    "        # and return the predicted class index (argmax over classes)\n",
    "        scores = self(x_col.view(np_tensor))\n",
    "        return int(np.argmax(scores, axis=0)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Softmax + Cross-Entropy (Forward + Seed Gradient)\n",
    "Computes:\n",
    "- **Softmax probabilities** column-wise from logits.\n",
    "- **Mean cross-entropy loss** across the batch.\n",
    "- **Gradient w.r.t. logits** (softmax âˆ’ one_hot) and stores it on `x.grad`, seeding backprop for the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmaxXEntropy(x, y):\n",
    "    \"\"\"\n",
    "    Compute softmax probabilities and cross-entropy loss, and store the gradient dL/dx in `x.grad`.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    x : np.ndarray\n",
    "        Logits (pre-softmax scores) with shape (num_classes, batch_size).\n",
    "        Each column corresponds to one sample in the batch.\n",
    "        NOTE: This function assumes that `x` can hold an attribute `.grad`\n",
    "        (e.g., a NumPy subclass or a wrapper that allows setting attributes).\n",
    "\n",
    "    y : np.ndarray\n",
    "        Integer class labels for the batch, shape (batch_size,) or (batch_size, 1).\n",
    "        Values must be in the range [0, num_classes-1]. If shape is (batch_size, 1),\n",
    "        it will be squeezed to (batch_size,).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    preds : np.ndarray\n",
    "        Softmax class probabilities with shape (num_classes, batch_size).\n",
    "        Each column sums to 1.\n",
    "    cost : float\n",
    "        Mean cross-entropy loss over the batch.\n",
    "\n",
    "    Side Effects\n",
    "    ------------\n",
    "    - Writes the gradient of the loss with respect to the logits into `x.grad`,\n",
    "      i.e., dL/dx = softmax(x) - one_hot(y), as implemented below.\n",
    "      NOTE: The current implementation does NOT divide the gradient by `batch_size`,\n",
    "      while the loss is averaged.\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    - Numerical stability: for production code, it is standard to shift logits by\n",
    "      their per-column maximum before exponentiation (log-sum-exp trick). Here we\n",
    "      keep the naive exponentiation to preserve the original code.\n",
    "    - If you encounter `log(0)` due to extremely small probabilities, adding a small\n",
    "      epsilon inside the log (e.g., 1e-12) is a common safeguard.\n",
    "    \"\"\"\n",
    "\n",
    "    # Determine batch size from logits tensor shaped (num_classes, batch_size)\n",
    "    batch_size = x.shape[1]\n",
    "\n",
    "    # Compute unnormalized scores via exponentiation (softmax numerator)\n",
    "    # NOTE: This is the naive form; in practice you would shift by max(x) per column for numerical stability.\n",
    "    exp_scores = np.exp(x)\n",
    "\n",
    "    # Normalize across classes to obtain probabilities (softmax denominator sums over axis=0 = per sample)\n",
    "    probs = exp_scores / exp_scores.sum(axis = 0)\n",
    "\n",
    "    # Keep a copy of probabilities to return as predictions\n",
    "    preds = probs.copy()\n",
    "\n",
    "    # ----- Cross-Entropy Loss -----\n",
    "    # Gather the predicted probability for the true class of each sample\n",
    "    y_hat = probs[y.squeeze(), np.arange(batch_size)]\n",
    "\n",
    "    # Compute mean cross-entropy loss over the batch\n",
    "    # NOTE: Adding a small epsilon inside log is common to avoid log(0), but we keep code unchanged here.\n",
    "    cost = np.sum(-np.log(y_hat)) / batch_size\n",
    "\n",
    "    # ----- Gradient w.r.t. logits (dL/dx) -----\n",
    "    # For softmax + cross-entropy, gradient is: probs - one_hot(y)\n",
    "    # Here we subtract 1 at the true class index for each sample\n",
    "    probs[y.squeeze(), np.arange(batch_size)] -= 1  # dl/dx\n",
    "\n",
    "    # Store gradient in x.grad for backpropagation through previous layers\n",
    "    # NOTE: If loss is averaged over batch, dividing by batch_size here is typical; preserved as-is per base code.\n",
    "    x.grad = probs.copy()\n",
    "\n",
    "    # Return probabilities (preds) and scalar loss (cost)\n",
    "    return preds, cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training Loop\n",
    "Trains for a fixed number of epochs:\n",
    "1. Iterate mini-batches (shuffled).\n",
    "2. Forward to get logits.\n",
    "3. Compute **softmax cross-entropy** and seed `logits.grad`.\n",
    "4. Backpropagate through the model.\n",
    "5. Update parameters with SGD.\n",
    "6. Print epoch-level loss and validation accuracy for progress tracking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, epochs, mb_size=128, learning_rate = 1e-3):\n",
    "    # Train the model for a given number of epochs using mini-batch SGD.\n",
    "    # Notes:\n",
    "    # - 'scores' are the logits returned by the model.\n",
    "    # - softmaxXEntropy(scores, y) computes the loss and seeds scores.grad.\n",
    "    # - model.backward() backpropagates through all layers.\n",
    "    # - model.update(learning_rate) applies an SGD step to parameters.\n",
    "    for epoch in range(epochs):\n",
    "        # Iterate over training data in mini-batches\n",
    "        for i, (x, y) in enumerate(create_minibatches(mb_size, x_train, y_train)):\n",
    "            # Forward pass: model expects (features, batch), so we transpose the batch\n",
    "            scores = model(x.T.view(np_tensor))\n",
    "            # Compute softmax cross-entropy loss and seed gradient on the final scores\n",
    "            _, cost = softmaxXEntropy(scores, y)\n",
    "            # Backpropagate gradients through the network\n",
    "            model.backward()\n",
    "            # Update parameters with the chosen learning rate\n",
    "            model.update(learning_rate)\n",
    "        # Friendly, English summary per epoch (shows the last batch loss and current validation accuracy)\n",
    "        print(f\"Epoch {epoch+1}/{epochs} â€” Last batch loss: {cost:.4f} | \"\n",
    "              f\"Validation accuracy: {accuracy(x_val, y_val, mb_size)*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy Metric (Mini-batch Evaluation)\n",
    "Computes classification accuracy over `(X, y)` using mini-batches:\n",
    "- Runs forward passes (transposing batches to `(features, batch)`).\n",
    "- Compares `argmax` predictions with true labels.\n",
    "- Returns overall **accuracy = correct / total**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(x, y, mb_size):\n",
    "    # Initialize counters for correct predictions and total samples\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    # Iterate over the dataset in mini-batches\n",
    "    for i, (x, y) in enumerate(create_minibatches(mb_size, x, y)):\n",
    "        # Forward pass: the model expects input shaped as (features, batch), so we transpose\n",
    "        pred = model(x.T.view(np_tensor))\n",
    "\n",
    "        # Count how many predictions match the true labels\n",
    "        correct += np.sum(np.argmax(pred, axis=0) == y.squeeze())\n",
    "\n",
    "        # Increase the total number of processed samples (batch size)\n",
    "        total += pred.shape[1]\n",
    "\n",
    "    # Return overall accuracy as correct predictions divided by total samples\n",
    "    return correct/total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Definition and Hyperparameters\n",
    "Specifies the MLP architecture **784 â†’ 256 â†’ 128 â†’ 24** with ReLU activations, along with:\n",
    "\n",
    "- **Mini-batch size** (e.g., 512)\n",
    "- **Learning rate** (e.g., 1eâ€‘3)\n",
    "- **Epochs** (e.g., 25)\n",
    "\n",
    "These choices balance training speed and performance for ASL SignMNIST."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential_layers([Linear(784, 256), ReLU(), Linear(256, 128), ReLU(), Linear(128, 24)])\n",
    "mb_size = 512\n",
    "learning_rate = 1e-3\n",
    "epochs = 25\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25 â€” Last batch loss: 0.1792 | Validation accuracy: 76.60%\n",
      "Epoch 2/25 â€” Last batch loss: 0.0197 | Validation accuracy: 78.61%\n",
      "Epoch 3/25 â€” Last batch loss: 0.0104 | Validation accuracy: 79.11%\n",
      "Epoch 4/25 â€” Last batch loss: 0.0068 | Validation accuracy: 79.50%\n",
      "Epoch 5/25 â€” Last batch loss: 0.0050 | Validation accuracy: 80.06%\n",
      "Epoch 6/25 â€” Last batch loss: 0.0042 | Validation accuracy: 80.12%\n",
      "Epoch 7/25 â€” Last batch loss: 0.0026 | Validation accuracy: 79.95%\n",
      "Epoch 8/25 â€” Last batch loss: 0.0031 | Validation accuracy: 80.17%\n",
      "Epoch 9/25 â€” Last batch loss: 0.0023 | Validation accuracy: 80.34%\n",
      "Epoch 10/25 â€” Last batch loss: 0.0017 | Validation accuracy: 80.42%\n",
      "Epoch 11/25 â€” Last batch loss: 0.0018 | Validation accuracy: 80.15%\n",
      "Epoch 12/25 â€” Last batch loss: 0.0014 | Validation accuracy: 80.56%\n",
      "Epoch 13/25 â€” Last batch loss: 0.0015 | Validation accuracy: 80.42%\n",
      "Epoch 14/25 â€” Last batch loss: 0.0011 | Validation accuracy: 80.59%\n",
      "Epoch 15/25 â€” Last batch loss: 0.0010 | Validation accuracy: 80.56%\n",
      "Epoch 16/25 â€” Last batch loss: 0.0008 | Validation accuracy: 80.54%\n",
      "Epoch 17/25 â€” Last batch loss: 0.0008 | Validation accuracy: 80.56%\n",
      "Epoch 18/25 â€” Last batch loss: 0.0008 | Validation accuracy: 80.65%\n",
      "Epoch 19/25 â€” Last batch loss: 0.0009 | Validation accuracy: 80.56%\n",
      "Epoch 20/25 â€” Last batch loss: 0.0008 | Validation accuracy: 80.48%\n",
      "Epoch 21/25 â€” Last batch loss: 0.0008 | Validation accuracy: 80.65%\n",
      "Epoch 22/25 â€” Last batch loss: 0.0007 | Validation accuracy: 80.70%\n",
      "Epoch 23/25 â€” Last batch loss: 0.0006 | Validation accuracy: 80.48%\n",
      "Epoch 24/25 â€” Last batch loss: 0.0007 | Validation accuracy: 80.62%\n",
      "Epoch 25/25 â€” Last batch loss: 0.0008 | Validation accuracy: 80.56%\n"
     ]
    }
   ],
   "source": [
    "train(model, epochs, mb_size, learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nicely Formatted Test Accuracy\n",
    "Computes overall test accuracy via mini-batches and prints:\n",
    "\n",
    "- Percentage with two decimals (e.g., `86.73%`)\n",
    "- Decimal form (e.g., `0.8673`)\n",
    "\n",
    "This provides a clear, professional summary for reports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEST ACCURACY: 81.32% (0.8132)\n"
     ]
    }
   ],
   "source": [
    "# Compute test accuracy with mini-batches\n",
    "test_acc = accuracy(x_test, y_test, mb_size)\n",
    "\n",
    "# Nicely formatted output\n",
    "print(f\"TEST ACCURACY: {test_acc * 100:.2f}% ({test_acc:.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Test Samples (n Trials)\n",
    "Runs `n` random predictions on the **test set**:\n",
    "- Displays each image (28Ã—28).\n",
    "- Prints predicted vs. true letter and a **CORRECT/INCORRECT** status.\n",
    "- Ends with a short summary of correct counts and percentage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_random_samples(n=5, seed=None):\n",
    "    \"\"\"\n",
    "    Run n random prediction tests on the test set.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    n : int\n",
    "        Number of random samples to test.\n",
    "    seed : int or None\n",
    "        Optional random seed for reproducibility.\n",
    "    \"\"\"\n",
    "    if seed is not None:\n",
    "        np.random.seed(seed)\n",
    "\n",
    "    correct = 0\n",
    "\n",
    "    for t in range(1, n + 1):\n",
    "        # Pick a random test sample index\n",
    "        idx = np.random.randint(len(y_test))\n",
    "\n",
    "        # Visualize the corresponding 28x28 grayscale image\n",
    "        plot_number(x_test[idx].reshape(28, 28))\n",
    "\n",
    "        # Prepare the sample as a column vector (features, 1) and get the predicted class index\n",
    "        pred = model.predict(x_test[idx].reshape(-1, 1))\n",
    "\n",
    "        # Map class indices to letters for readability\n",
    "        pred_letter = alphabet[pred]\n",
    "        true_letter = alphabet[y_test[idx]]\n",
    "\n",
    "        # Determine if the prediction matches the ground truth\n",
    "        status = \"CORRECT\" if pred == y_test[idx] else \"INCORRECT\"\n",
    "        correct += (status == \"CORRECT\")\n",
    "\n",
    "        # Print a clear, uppercase status along with predicted vs. true labels\n",
    "        print(f\"[{t}/{n}] PREDICTION: {pred_letter} | TRUE: {true_letter} | STATUS: {status}\")\n",
    "\n",
    "    # Summary of the mini test session\n",
    "    print(f\"\\nSUMMARY: {correct}/{n} correct \"\n",
    "          f\"({100.0 * correct / max(1, n):.2f}% accuracy over random samples)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/10] PREDICTION: s | TRUE: s | STATUS: CORRECT\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2/10] PREDICTION: f | TRUE: f | STATUS: CORRECT\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3/10] PREDICTION: r | TRUE: r | STATUS: CORRECT\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4/10] PREDICTION: q | TRUE: q | STATUS: CORRECT\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5/10] PREDICTION: h | TRUE: h | STATUS: CORRECT\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6/10] PREDICTION: m | TRUE: m | STATUS: CORRECT\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7/10] PREDICTION: w | TRUE: x | STATUS: INCORRECT\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[8/10] PREDICTION: h | TRUE: h | STATUS: CORRECT\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[9/10] PREDICTION: c | TRUE: c | STATUS: CORRECT\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10/10] PREDICTION: i | TRUE: i | STATUS: CORRECT\n",
      "\n",
      "SUMMARY: 9/10 correct (90.00% accuracy over random samples)\n"
     ]
    }
   ],
   "source": [
    "test_random_samples(n=10, seed=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions â€” Activity 1b: Implementing a Fully Connected Network for the Kaggle ASL Dataset\n",
    "\n",
    "This collaborative activity allowed our team to implement a complete Fully Connected Network (FCN) from scratch using NumPy, applying the concepts learned in class to the Kaggle ASL (American Sign Language) dataset. By developing every component manuallyâ€”including forward passes, backpropagation, gradient calculations, minibatch training, and parameter updatesâ€”we deepened our understanding of how neural networks function internally beyond highâ€‘level frameworks like Keras or PyTorch.\n",
    "\n",
    "### Key Technical Insights\n",
    "\n",
    "- **Network Architecture:**  \n",
    "  Our final architecture consisted of three Linear layers with ReLU activations in between:  \n",
    "  **784 â†’ 256 â†’ 128 â†’ 24**, a structure wellâ€‘suited for the flattened 28Ã—28 grayscale ASL images.  \n",
    "  We selected **He initialization** to improve training stability with ReLU.\n",
    "\n",
    "- **Training Configuration:**  \n",
    "  - Miniâ€‘batch size: **512**  \n",
    "  - Learning rate: **1eâ€‘3**  \n",
    "  - Epochs: **25**  \n",
    "  - Loss function: **Softmax + Crossâ€‘Entropy (implemented manually)**  \n",
    "  - Optimizer: **Vanilla SGD**\n",
    "\n",
    "- **Results Obtained:**  \n",
    "  After training for 25 epochs, our team achieved the following performance:\n",
    "  - **Validation accuracy:** ~76â€“80%  \n",
    "  - **Final test accuracy:** **â‰ˆ81%**  \n",
    "  These results meet and exceed the activity requirement of **â‰¥70% accuracy**.\n",
    "\n",
    "- **Model Behavior:**  \n",
    "  We observed a stable decrease in loss over epochs and consistent improvement in validation accuracy, which confirmed that our backpropagation and gradient updates were implemented correctly. Testing random samples also showed strong qualitative performance, with the model predicting the correct ASL letter the majority of the time.\n",
    "\n",
    "\n",
    "### Lessons Learned\n",
    "\n",
    "1. **Implementing backpropagation manually** significantly strengthens the conceptual understanding of how gradients flow layer by layer.  \n",
    "2. **Data normalization and proper weight initialization** (He) make a substantial difference in convergence speed and model stability.  \n",
    "3. **Batching and shuffling** are essential to avoid overfitting and ensure smooth gradient updates.  \n",
    "4. Building even a â€œsimpleâ€ FCN from scratch reveals how much abstraction highâ€‘level libraries provide.\n",
    "\n",
    "Overall, this activity was an excellent opportunity to strengthen both theoretical understanding and practical implementation skills of neural networks at a low level."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "amlm-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
